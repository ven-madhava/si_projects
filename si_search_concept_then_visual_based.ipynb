{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# a single notebook that serves only search purpose\n",
    "##\n",
    "\n",
    "1. get input image & do necessary preprocessing\n",
    "2. detect concepts in it -- output is multiclass prediction\n",
    "3. filter results from base folder based on multiclass prediction\n",
    "4. sort the filtered results based on visual similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports - done\n",
    "# ------------------------\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import h5py\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import math\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import save_image\n",
    "import torch.optim as optim\n",
    "\n",
    "from __future__ import print_function\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "# scipy related\n",
    "# -------------\n",
    "import scipy\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage import filters\n",
    "from scipy import misc\n",
    "\n",
    "# working now\n",
    "# -----------\n",
    "#import skimage.io\n",
    "#from skimage.transform import rotate, AffineTransform, warp\n",
    "#from skimage.util import random_noise\n",
    "#from skimage.filters import gaussian\n",
    "#from skimage import transform as tf\n",
    "\n",
    "\n",
    "# neccessary imports for imgaug\n",
    "# ------------------------------\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
    "\n",
    "###\n",
    "###\n",
    "%matplotlib inline\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp\n",
    "\n",
    "\n",
    "# printing platform info\n",
    "# ----------------------\n",
    "import platform\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize pool function\n",
    "# --------------------\n",
    "\n",
    "def resize_pool(x,h,w):\n",
    "    \n",
    "    # 0. global inits\n",
    "    # ---------------\n",
    "    global x_in\n",
    "    x_in = x\n",
    "    \n",
    "    global h_in,w_in\n",
    "    h_in,w_in = h,w\n",
    "    \n",
    "    global x_out\n",
    "    x_out = np.zeros((x.shape[0],h,w,x.shape[3]))\n",
    "    \n",
    "    # 1. calling resize function across multiprocessing pool\n",
    "    # ------------------------------------------------------\n",
    "    pool = ThreadPool(5) \n",
    "    pool.map(resize_single, list(range(x.shape[0])))\n",
    "    \n",
    "    # sanity\n",
    "    # ------\n",
    "    print('done with ' + str(len(x)) +' images. access at global x_out.')\n",
    "    \n",
    "    # closing pools\n",
    "    # -------------\n",
    "    pool.terminate()\n",
    "    pool.join()\n",
    "    \n",
    "\n",
    "\n",
    "# single function\n",
    "# ----------------\n",
    "def resize_single(index):\n",
    "    \n",
    "    # 0. getting in globls\n",
    "    # --------------------\n",
    "    global x_in\n",
    "    global h_in,w_in\n",
    "    global x_out\n",
    "    \n",
    "    # resize and switch in\n",
    "    # -------------------\n",
    "    curr_img = cv2.resize(x_in[index],(w_in,h_in))\n",
    "    curr_img = curr_img.reshape(h_in,w_in,x_in.shape[3])\n",
    "    x_out[index] = curr_img\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function to return filtered inds\n",
    "# ---------------------------------------\n",
    "\n",
    "def return_filtered_concepts_inds(pred_input_in,pred_db_in):\n",
    "\n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    pred_input = pred_input_in.data.numpy()\n",
    "    pred_input[pred_input < 0.5] = 0\n",
    "    pred_input[pred_input >= 0.5] = 1\n",
    "    \n",
    "    \n",
    "    pred_db = pred_db_in.data.numpy()\n",
    "    pred_db[pred_db < 0.5] = 0\n",
    "    pred_db[pred_db >= 0.5] = 1\n",
    "    \n",
    "    \n",
    "    inds_list = []\n",
    "    \n",
    "    # main iter\n",
    "    # ---------\n",
    "    for i in range(pred_input.shape[0]):\n",
    "\n",
    "        # step 1\n",
    "        # at each input image prediction level\n",
    "        # just pick examples where sum of positions of zeros should be same\n",
    "        # ---------------------------------------------------------------\n",
    "        zero_pos = (pred_input[i]==0).astype(int) * pred_db\n",
    "        zero_pos_sum = np.sum(zero_pos, axis = 1)\n",
    "        filtered_inds = np.argwhere(zero_pos_sum == 0)[:,0]\n",
    "        inds_list.append(filtered_inds)\n",
    "        \n",
    "    \n",
    "    # final return\n",
    "    # ------------\n",
    "    return inds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERIC function to calculate conv outsize\n",
    "# -------------------------------------------- \n",
    "def outsize_conv(n_H,n_W,f,s,pad):\n",
    "    \n",
    "    h = ((n_H - f + (2*pad))/s) + 1\n",
    "    w = ((n_W - f + (2*pad))/s) + 1\n",
    "    return h,w\n",
    "    \n",
    "    \n",
    "# GENERIC function to calculate upconv outsize\n",
    "# --------------------------------------------    \n",
    "def outsize_upconv(h,w,f,s,p):\n",
    "    hout = (h-1)*s - 2*p + f\n",
    "    wout = (w-1)*s - 2*p + f\n",
    "    return hout, wout\n",
    "\n",
    "\n",
    "\n",
    "# GENERIC - initialises weights for a NN\n",
    "# --------------------------------------\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "        \n",
    "# GENERIC - change an torch image to numpy image\n",
    "# ----------------------------------------------\n",
    "def to_numpy_image(xin):\n",
    "    \n",
    "    try:\n",
    "        xin = xin.data.numpy()\n",
    "    except:\n",
    "        xin = xin.numpy()\n",
    "    \n",
    "    xout = np.swapaxes(xin,1,2)\n",
    "    xout = np.swapaxes(xout,2,3)\n",
    "    \n",
    "    # returns axes swapped numpy images\n",
    "    # ---------------------------------\n",
    "    return xout       \n",
    "\n",
    "\n",
    "\n",
    "# GENERIC - converts numpy images to torch tensors for training\n",
    "# -------------------------------------------------------------\n",
    "def setup_image_tensor(xin):\n",
    "    xout = np.swapaxes(xin,1,3)\n",
    "    xout = np.swapaxes(xout,2,3)\n",
    "    \n",
    "    # returns axes swapped torch tensor\n",
    "    # ---------------------------------\n",
    "    xout = torch.from_numpy(xout)\n",
    "    return xout.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to load a saved model\n",
    "# --------------------------------\n",
    "\n",
    "def load_saved_model_function(path, use_cuda):\n",
    "    \n",
    "    \n",
    "    ''' path = /folder1/folder2/model_ae.tar format'''\n",
    "    \n",
    "    # 1. loading full model\n",
    "    # ---------------------\n",
    "    model = torch.load(path.replace('.tar','_MODEL.tar'))\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,model.parameters()))\n",
    "    \n",
    "    # 2. Applying state dict\n",
    "    # ----------------------\n",
    "    if use_cuda == True:\n",
    "        \n",
    "        # loads to GPU\n",
    "        # ------------\n",
    "        checkpoint = torch.load(path)\n",
    "        \n",
    "    else:\n",
    "        # loads to CPU\n",
    "        # ------------\n",
    "        checkpoint = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        \n",
    "        \n",
    "    # loading checkpoint\n",
    "    # -------------------\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # loading optimizer\n",
    "    # -----------------\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if use_cuda == True:\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.cuda()\n",
    "            \n",
    "            \n",
    "            \n",
    "    # loading other stuff\n",
    "    # -------------------\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    loss_mode = checkpoint['loss_mode']\n",
    "    \n",
    "    return model, optimizer, epoch, loss, loss_mode\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function steps\n",
    "# -------------------\n",
    "def score_cam(model_mpn,model,x_in_scam_in,x_in_print,layer,norm_mode):\n",
    "    \n",
    "    \n",
    "    ###### MAIN FUNCTION STEPS #######\n",
    "    ##################################\n",
    "    \n",
    "    # sanity\n",
    "    # ------\n",
    "    assert torch.mean(x_in_scam_in) > 1,'Error: images in needs to be in 0-255 range.'\n",
    "    \n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    x_in_scam = x_in_scam_in/torch.max(x_in_scam_in)\n",
    "    model = model.eval()\n",
    "    model_mpn = model_mpn.eval()\n",
    "    x_in_scam_np = to_numpy_image(x_in_scam)\n",
    "    out_dict = {}\n",
    "    img_in_h, img_in_w = x_in_scam.size()[2], x_in_scam.size()[3]\n",
    "    \n",
    "\n",
    "    # 1. get the feature maps & predictions\n",
    "    # --------------------------------------\n",
    "    x_in_masks = model_mpn(x_in_scam)\n",
    "    x_in_masks[x_in_masks < 0.5] = 0\n",
    "    x_in_masks[x_in_masks >= 0.5] = 1\n",
    "    f_maps = model.score_cam_fmaps(x_in_scam*x_in_masks,layer)\n",
    "    pred = model.eval()(x_in_scam*x_in_masks)\n",
    "    pred[pred < 0.5] = 0\n",
    "    pred[pred >= 0.5] = 1\n",
    "\n",
    "\n",
    "    # 2. normalise each feature map to 0-1\n",
    "    # unfortunately we have to loop through only\n",
    "    # also remember feature maps are > 0 since they're after RELU\n",
    "    # this means we could just to value/max\n",
    "    # we will include all ops within this for loop :(\n",
    "    # itering each input image\n",
    "    # ------------------------\n",
    "    for i in range(f_maps.size()[0]):\n",
    "\n",
    "        # 2.1 local inits\n",
    "        # ---------------\n",
    "        out_dict[i] = {}\n",
    "        curr_fmap = f_maps[i]\n",
    "        curr_class_inds = list(np.argwhere(pred[i].detach().numpy()==1)[:,0])\n",
    "\n",
    "        # continue only if there's a prediction\n",
    "        # -------------------------------------\n",
    "        if len(curr_class_inds) > 0:\n",
    "\n",
    "            # 2.2 computing max and min values along channel axis\n",
    "            # ---------------------------------------------------\n",
    "            mxvals = torch.max(torch.max(curr_fmap,1).values,1).values\n",
    "            mxvals = mxvals.view(curr_fmap.size()[0],1,1)\n",
    "            mnvals = torch.min(torch.min(curr_fmap,1).values,1).values\n",
    "            mnvals = mnvals.view(curr_fmap.size()[0],1,1)\n",
    "\n",
    "            # 2.3 final normalisation\n",
    "            # ------------------------\n",
    "            if norm_mode == 'max':\n",
    "                curr_fmap_norm = curr_fmap/(mxvals + 0.0001)\n",
    "            elif norm_mode == 'minmax':\n",
    "                curr_fmap_norm = (curr_fmap - mnvals)/ (mxvals - mnvals + 0.0001)\n",
    "\n",
    "\n",
    "            # 2.4 resizing ops\n",
    "            # ----------------\n",
    "            curr_fmap_norm = curr_fmap_norm.view(1,curr_fmap_norm.size()[0],curr_fmap_norm.size()[1],curr_fmap_norm.size()[2])\n",
    "            curr_fmap_norm_np = to_numpy_image(curr_fmap_norm.data)\n",
    "            curr_fmap_norm_np = cv2.resize(curr_fmap_norm_np[0],(img_in_w,img_in_h))\n",
    "            #curr_fmap_norm_np[curr_fmap_norm_np < 0.5] = 0\n",
    "            #curr_fmap_norm_np[curr_fmap_norm_np >= 0.5] = 1\n",
    "            ## output here is of shape (img_h,img_w,no_channels) i.e., (95,95,256) etc..\n",
    "            ##\n",
    "\n",
    "\n",
    "            # 2.5 itering through each channel and masking input image\n",
    "            # we are concerned with classes that are predicted by the model only\n",
    "            # we want to filter down activation maps based on classes predicted only\n",
    "            # ----------------------------------------------------------------------\n",
    "            for each_ch in range(curr_fmap_norm_np.shape[2]):\n",
    "\n",
    "                # mask ops\n",
    "                # --------\n",
    "                curr_fmap_norm_np_channel = curr_fmap_norm_np[:,:,each_ch]\n",
    "                curr_fmap_norm_np_channel = curr_fmap_norm_np_channel.reshape(img_in_h,img_in_w,1)\n",
    "                curr_masked_image = curr_fmap_norm_np_channel * x_in_scam_np[i]\n",
    "                curr_masked_image = curr_masked_image.reshape(1,img_in_h,img_in_w,1)\n",
    "                curr_masked_image = curr_masked_image/(np.max(curr_masked_image) + 0.0001)\n",
    "\n",
    "                # prediction pass\n",
    "                # ---------------\n",
    "                curr_masked_image_pred = model(Variable(setup_image_tensor(curr_masked_image)).float())\n",
    "                curr_masked_image_pred_max_label = torch.argmax(curr_masked_image_pred)\n",
    "                curr_masked_image_pred_max_weight = torch.max(curr_masked_image_pred).item()\n",
    "\n",
    "                # adding weight & map to dict\n",
    "                # ---------------------------\n",
    "                if curr_masked_image_pred_max_label in curr_class_inds:\n",
    "                    try:\n",
    "                        out_dict[i][curr_masked_image_pred_max_label.item()]['act'] += curr_masked_image_pred_max_weight * curr_fmap_norm_np_channel\n",
    "                    except:\n",
    "                        out_dict[i][curr_masked_image_pred_max_label.item()] = {}\n",
    "                        out_dict[i][curr_masked_image_pred_max_label.item()]['act'] = curr_masked_image_pred_max_weight * curr_fmap_norm_np_channel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # 2.6 we have to iter through dicts\n",
    "            # ---------------------------------\n",
    "            for keys_labels in out_dict[i]:\n",
    "\n",
    "                # dicts are in format\n",
    "                # d[image_index] has keys ['class_label'] index positions [0,2] etc\n",
    "                # d[image_index][class_label] has ['act'] which has non-normalised activation map\n",
    "                # for this class_label\n",
    "                # -------------------------------------\n",
    "                #norm_act_map = (out_dict[i][keys_labels]['act'] - np.min(out_dict[i][keys_labels]['act']))/(np.max(out_dict[i][keys_labels]['act']) - np.min(out_dict[i][keys_labels]['act']) + 0.0001)\n",
    "                norm_act_map = out_dict[i][keys_labels]['act']/(np.max(out_dict[i][keys_labels]['act']) + 0.0001)\n",
    "                #norm_act_map = out_dict[i][keys_labels]['act']\n",
    "\n",
    "                # heatmap ops\n",
    "                # -----------\n",
    "                heat_map = cv2.applyColorMap(np.uint8(255*(1-norm_act_map)), cv2.COLORMAP_JET)\n",
    "                heat_map = heat_map/np.max(heat_map)\n",
    "                curr_visual_image = x_in_print[i]/np.max(x_in_print[i])\n",
    "                overlayed_img = heat_map * 0.5 + curr_visual_image * 0.5\n",
    "                \n",
    "                # adding to dict\n",
    "                # --------------\n",
    "                out_dict[i][keys_labels]['heatmap'] = overlayed_img\n",
    "                \n",
    "            \n",
    "            \n",
    "    # printing results by default\n",
    "    # ---------------------------\n",
    "    for i in out_dict.keys():\n",
    "    \n",
    "        # sanity\n",
    "        # ------\n",
    "        print('image # ' + str(i))\n",
    "        print('no classes here : ' + str(len(out_dict[i].keys())))\n",
    "\n",
    "        # iter thru classes\n",
    "        # -----------------\n",
    "        for class_inds in out_dict[i]:\n",
    "\n",
    "            # print class\n",
    "            # -----------\n",
    "            print('class: ' + str(class_labels[class_inds]))\n",
    "            plt.imshow(out_dict[i][class_inds]['heatmap'])\n",
    "            plt.show()\n",
    "\n",
    "        # sanity\n",
    "        # ------\n",
    "        print('**************')\n",
    "    \n",
    "    \n",
    "    # final return\n",
    "    # ------------\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple forward pass function\n",
    "# ------------------------------\n",
    "\n",
    "\n",
    "def simple_forward_pass_pool(xin,input_is_image,model):\n",
    "    \n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    global model_global\n",
    "    model_global = model\n",
    "    \n",
    "    global x_in_global\n",
    "    x_in_global = copy.deepcopy(xin)\n",
    "    \n",
    "    # 1. setting up y_out size\n",
    "    # ------------------------\n",
    "    global y_out_global\n",
    "    sz = list(model(xin[0:2]).size())\n",
    "    final_size = sz[1:]\n",
    "    final_size = tuple([xin.size()[0]] + final_size)\n",
    "    y_out_global = torch.zeros((final_size))\n",
    "    \n",
    "    \n",
    "    # 1.1 sanity\n",
    "    # ----------\n",
    "    if input_is_image == True:\n",
    "        assert torch.mean(x_in_global) > 1, 'Error: Data already in 0-1 range'\n",
    "        x_in_global = x_in_global/torch.max(x_in_global)\n",
    "        \n",
    "    \n",
    "    # 2. calling pool function\n",
    "    # ------------------------\n",
    "    pool = ThreadPool(10)\n",
    "    pool.map(simple_forward_pass_single, list(range(xin.size()[0])))\n",
    "    print('Done with forward pass of around ' + str(xin.size()[0]) + ' examples. Access them at global y_out_global.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def simple_forward_pass_single(i):\n",
    "    \n",
    "    # 0. global inits\n",
    "    # ---------------\n",
    "    global model_global\n",
    "    global x_in_global\n",
    "    global y_out_global\n",
    "    \n",
    "    # 1. ops\n",
    "    # ------\n",
    "    curr_example = x_in_global[i]\n",
    "    curr_example = curr_example.view(1,curr_example.size()[0],curr_example.size()[1],curr_example.size()[2])\n",
    "    curr_out = model_global(curr_example)\n",
    "    y_out_global[i] = curr_out[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function to read a single image\n",
    "# --------------------------------------\n",
    "\n",
    "def create_dataset_from_folder_all(infolder,resize,gray_mode,n_h,n_w):\n",
    "    \n",
    "    # 0. global initialisations\n",
    "    # -------------------------\n",
    "    global index_list\n",
    "    index_list = []\n",
    "    \n",
    "    global resize_flag\n",
    "    resize_flag = resize\n",
    "    \n",
    "    global gb_in_folder\n",
    "    gb_in_folder = infolder\n",
    "\n",
    "    global counter\n",
    "    counter = 0\n",
    "    \n",
    "    global new_h\n",
    "    new_h = n_h\n",
    "    \n",
    "    global new_w\n",
    "    new_w = n_w\n",
    "    \n",
    "    global image_list\n",
    "    image_list_jpg = [f for f in listdir(infolder) if isfile(join(infolder, f)) and '.jpg' in f.lower()]\n",
    "    image_list_png = [f for f in listdir(infolder) if isfile(join(infolder, f)) and '.png' in f.lower()]\n",
    "    image_list = image_list_jpg + image_list_png\n",
    "\n",
    "    global x_images_dataset\n",
    "    x_images_dataset = np.zeros((len(image_list),new_h,new_w,3), dtype='uint8')\n",
    "    \n",
    "    global x_images_dataset_gray\n",
    "    x_images_dataset_gray = np.zeros((len(image_list),new_h,new_w), dtype='uint8')\n",
    "    \n",
    "    global x_images_dataset_edge\n",
    "    x_images_dataset_edge = np.zeros((len(image_list),new_h,new_w,1))\n",
    "    \n",
    "    \n",
    "    # 1.1 sanity assertion\n",
    "    # -------------------\n",
    "    assert len(image_list) > 0, 'No images in the folder'\n",
    "    \n",
    "    \n",
    "    # 2. calling resize function across multiprocessing pool\n",
    "    # ------------------------------------------------------\n",
    "    pool = ThreadPool(5) \n",
    "    pool.map(create_dataset_from_folder_single, list(range(len(image_list))))\n",
    "    \n",
    "    # 2.1 sanity assert\n",
    "    # -----------------\n",
    "    assert x_images_dataset.shape[0] == x_images_dataset_gray.shape[0], 'RGB and Grayscale images have different numbers of images!'\n",
    "    \n",
    "    # 3. filtering out the dataset\n",
    "    # ----------------------------\n",
    "    print('Len at start: ' + str(x_images_dataset.shape))\n",
    "    x_images_dataset = x_images_dataset[index_list]\n",
    "    x_images_dataset_gray = x_images_dataset_gray[index_list]\n",
    "    x_images_dataset_edge = x_images_dataset_edge[index_list]\n",
    "    \n",
    "    \n",
    "    # hard normalising grayscale dataset by individual means\n",
    "    # ------------------------------------------------------\n",
    "    if gray_mode == 'bw':\n",
    "        \n",
    "        # hard b/w single channel\n",
    "        # -----------------------        \n",
    "        mn = np.mean(np.mean(x_images_dataset_gray, axis = 1), axis = 1)\n",
    "        mn = mn.reshape(mn.shape[0],1,1)\n",
    "        x_images_dataset_gray[x_images_dataset_gray < mn] = 0\n",
    "        x_images_dataset_gray[x_images_dataset_gray >= mn] = 255\n",
    "        x_images_dataset_gray = x_images_dataset_gray.reshape(x_images_dataset_gray.shape[0],new_h,new_w,1)\n",
    "        \n",
    "    elif gray_mode == 'gray_3':\n",
    "        \n",
    "        # grayscale 3 channel\n",
    "        # -------------------\n",
    "        x_images_dataset_gray = x_images_dataset_gray.reshape(x_images_dataset_gray.shape[0],new_h,new_w,1)\n",
    "        x_images_dataset_gray = np.concatenate((x_images_dataset_gray,x_images_dataset_gray,x_images_dataset_gray), axis= 3)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # grayscale 1 channel\n",
    "        # -------------------\n",
    "        x_images_dataset_gray = x_images_dataset_gray.reshape(x_images_dataset_gray.shape[0],new_h,new_w,1)\n",
    "        \n",
    "\n",
    "    print('Len after filtering: ' + str(x_images_dataset.shape))\n",
    "    print('Done creating dataset of around ' + str(counter) + ' images. Access them at global x_images_dataset, x_images_dataset_gray, x_images_dataset_edge.')\n",
    "    \n",
    "    # closing pools\n",
    "    # -------------\n",
    "    pool.terminate()\n",
    "    pool.join()\n",
    "    \n",
    "\n",
    "def create_dataset_from_folder_single(i):\n",
    "    \n",
    "    # 0. calling global variables\n",
    "    # ---------------------------\n",
    "    global gb_in_folder\n",
    "    global counter\n",
    "    global new_h\n",
    "    global new_w\n",
    "    global x_images_dataset\n",
    "    global x_images_dataset_gray\n",
    "    global image_list\n",
    "    global resize_flag\n",
    "    global x_images_dataset_edge\n",
    "    \n",
    "    \n",
    "    # 1. ops\n",
    "    # ------\n",
    "    try:\n",
    "        name = image_list[i]\n",
    "        img_main = cv2.imread(join(gb_in_folder, name))\n",
    "        img = cv2.cvtColor(copy.deepcopy(img_main), cv2.COLOR_BGR2RGB)\n",
    "        img_gray = cv2.cvtColor(copy.deepcopy(img_main), cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # resizing ops\n",
    "        # -----------\n",
    "        if resize_flag == True:\n",
    "            img = cv2.resize(img, (new_w,new_h))\n",
    "            img_gray = cv2.resize(img_gray, (new_w,new_h))\n",
    "            \n",
    "\n",
    "        # 5. by default building edge images\n",
    "        # ----------------------------------\n",
    "        blurred = cv2.GaussianBlur(img_gray.reshape(new_h,new_w).astype('uint8'), (7, 7), 0)\n",
    "        edged = cv2.Canny(blurred, 50, 150)\n",
    "        edged = edged.reshape(new_h,new_w,1)\n",
    "        \n",
    "        # final assignments\n",
    "        # ------------------\n",
    "        x_images_dataset[i] = img\n",
    "        x_images_dataset_gray[i] = img_gray\n",
    "        x_images_dataset_edge[i] = edged\n",
    "        \n",
    "        counter += 1\n",
    "        index_list.append(i)\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        # do nothing\n",
    "        ##\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that will return final latents for similarity function \n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "def final_latents(f_maps,kernel_stride_dims,pool_mode,aggregate_pool_maps):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    1. input is a dict with keys - \n",
    "    \n",
    "    a. f_maps - list of feature maps (m,c,h,w) on which pooling functions can be run for latent computation\n",
    "    b. kernel_dims - list of kernel dimensions that will be used for pooling on feature maps\n",
    "    c. pool_mode - 'max', 'avg' or 'both'\n",
    "    d. aggregate_pool_maps - either sum up pool values or not\n",
    "    \n",
    "    2. output will be a dict with final latents to be input to similarity function\n",
    "    3. output latents will be L2 normalized\n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    \n",
    "    # 0. initialisations\n",
    "    # ------------------\n",
    "    latents_out = []\n",
    "    \n",
    "    \n",
    "    # 1. computing rmac latents\n",
    "    # -------------------------\n",
    "    for each_fmap in f_maps:\n",
    "        for each_kernel_dim in kernel_stride_dims:\n",
    "            curr_latent = return_ms_rmac([each_fmap],pool_mode,each_kernel_dim,aggregate_pool_maps).data.numpy()\n",
    "            \n",
    "            # L2 normalization\n",
    "            # ----------------\n",
    "            #curr_latent = curr_latent / np.linalg.norm(curr_latent)\n",
    "            \n",
    "            # appending\n",
    "            # ----------\n",
    "            latents_out.append(curr_latent)\n",
    "            print('done at kernel ' + str(each_kernel_dim) + '. latent size: ' + str(curr_latent.shape))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # final return\n",
    "    # ------------\n",
    "    return latents_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "# -------------\n",
    "\n",
    "def return_ms_rmac(fmaps_list,pool_mode,kernel_dims,aggregate_pool_maps):\n",
    "    \n",
    "    '''\n",
    "    ref: https://www.researchgate.net/publication/313465134_MS-RMAC_Multiscale_Regional_Maximum_Activation_of_Convolutions_for_Image_Retrieval\n",
    "    \n",
    "    1. takes as input fmaps tuple with feature masps of size (m,k,h,w) each where k = no_channels at each layer\n",
    "    -- input is ((m1,k1,h1,w1), (m2,k2,h2,w2),...)\n",
    "    -- pool_mode = 'max','avg','both'\n",
    "    -- kernel_dims = None or (f,s)\n",
    "    -- aggregate_pool_maps - if true, we will sum across channels, else leave as it is\n",
    "    \n",
    "    2. works out 3 scales of MAC kernel sizes for each feature map in tuple\n",
    "    3. computes MAC i.e., maximum activations convolutions & aggregates them\n",
    "    4. returns concatenated (m,K) vector where K = k1 + k2 + k3.. i.e., no_channels at each layer in fmaps in tuple\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 0. initialisations\n",
    "    # -------------------\n",
    "    assert type(fmaps_list) == list, 'Type error: input feature maps must be a list'\n",
    "    \n",
    "    \n",
    "    # 1. looping through fmap tuple\n",
    "    # -----------------------------\n",
    "    for fmap in fmaps_list:\n",
    "        \n",
    "        # 1.1 checking kernel initialisations\n",
    "        # -----------------------------------\n",
    "        if kernel_dims == None:\n",
    "            \n",
    "            # 1.1.1 we will be using preset scaled regions - computing f,s\n",
    "            # ------------------------------------------------------------\n",
    "            h,w = fmap.size()[2],fmap.size()[3]\n",
    "\n",
    "            # scale 1 : w is same, h = h/2, stride = curr_h/3\n",
    "            ##\n",
    "            l1_h = int(h/2)\n",
    "            l1_w = w\n",
    "            l1_s = int(l1_h/2)\n",
    "\n",
    "            # scale 2 : h = h/2, w = 2w/3, stride = curr_w/2\n",
    "            ##\n",
    "            l2_h = int(h/2)\n",
    "            l2_w = int(2*w/3)\n",
    "            l2_s = int(l2_w/2)\n",
    "\n",
    "            # scale 3 : h = 2h/5, w = w/2, stride = curr_w/2\n",
    "            ##\n",
    "            l3_h = int(2*h/5)\n",
    "            l3_w = int(w/2)\n",
    "            l3_s = int(l3_w/2)\n",
    "            \n",
    "            \n",
    "            # 1.1.1.2 forward pass to get pool maps\n",
    "            # --------------------------------------\n",
    "            if pool_mode == 'max':\n",
    "                \n",
    "                # computing mx pool @ scale 1\n",
    "                # ---------------------------\n",
    "                pl_l1 =  nn.Sequential(*[nn.MaxPool2d((l1_h,l1_w), stride=l1_s)])\n",
    "                l1_pool_map = pl_l1(fmap)\n",
    "                l1_pool_map = torch.sum(torch.sum(l1_pool_map, 2),2)\n",
    "                \n",
    "                # computing mx pool @ scale 2\n",
    "                # ---------------------------\n",
    "                pl_l2 =  nn.Sequential(*[nn.MaxPool2d((l2_h,l2_w), stride=l2_s)])\n",
    "                l2_pool_map = pl_l2(fmap)\n",
    "                l2_pool_map = torch.sum(torch.sum(l2_pool_map, 2),2)\n",
    "                \n",
    "                # computing mx pool @ scale 3\n",
    "                # ---------------------------\n",
    "                pl_l3 =  nn.Sequential(*[nn.MaxPool2d((l3_h,l3_w), stride=l3_s)])\n",
    "                l3_pool_map = pl_l3(fmap)\n",
    "                l3_pool_map = torch.sum(torch.sum(l3_pool_map, 2),2)\n",
    "                \n",
    "                \n",
    "                # NOT concatenating -- summing\n",
    "                # -----------------------------\n",
    "                combined_pool_map = l1_pool_map + l2_pool_map + l3_pool_map\n",
    "                \n",
    "                \n",
    "            \n",
    "            elif pool_mode == 'avg':\n",
    "                \n",
    "\n",
    "                # computing avg pool @ scale 1\n",
    "                # ---------------------------\n",
    "                pl_l1 =  nn.Sequential(*[nn.AvgPool2d((l1_h,l1_w), stride=l1_s)])\n",
    "                l1_pool_map = pl_l1(fmap)\n",
    "                l1_pool_map = torch.sum(torch.sum(l1_pool_map, 2),2)\n",
    "                \n",
    "                # computing avg pool @ scale 2\n",
    "                # ---------------------------\n",
    "                pl_l2 =  nn.Sequential(*[nn.AvgPool2d((l2_h,l2_w), stride=l2_s)])\n",
    "                l2_pool_map = pl_l2(fmap)\n",
    "                l2_pool_map = torch.sum(torch.sum(l2_pool_map, 2),2)\n",
    "                \n",
    "                # computing avg pool @ scale 3\n",
    "                # ---------------------------\n",
    "                pl_l3 =  nn.Sequential(*[nn.AvgPool2d((l3_h,l3_w), stride=l3_s)])\n",
    "                l3_pool_map = pl_l3(fmap)\n",
    "                l3_pool_map = torch.sum(torch.sum(l3_pool_map, 2),2)\n",
    "                \n",
    "                # NOT concatenating -- summing\n",
    "                # ----------------------------\n",
    "                combined_pool_map = l1_pool_map + l2_pool_map + l3_pool_map\n",
    "                \n",
    "                \n",
    "            elif pool_mode == 'both':\n",
    "                \n",
    "                # computing mx pool @ scale 1\n",
    "                # ---------------------------\n",
    "                pl_l1 =  nn.Sequential(*[nn.MaxPool2d((l1_h,l1_w), stride=l1_s)])\n",
    "                l1_pool_map = pl_l1(fmap)\n",
    "                l1_pool_map = torch.sum(torch.sum(l1_pool_map, 2),2)\n",
    "                \n",
    "                # computing mx pool @ scale 2\n",
    "                # ---------------------------\n",
    "                pl_l2 =  nn.Sequential(*[nn.MaxPool2d((l2_h,l2_w), stride=l2_s)])\n",
    "                l2_pool_map = pl_l2(fmap)\n",
    "                l2_pool_map = torch.sum(torch.sum(l2_pool_map, 2),2)\n",
    "                \n",
    "                # computing mx pool @ scale 3\n",
    "                # ---------------------------\n",
    "                pl_l3 =  nn.Sequential(*[nn.MaxPool2d((l3_h,l3_w), stride=l3_s)])\n",
    "                l3_pool_map = pl_l3(fmap)\n",
    "                l3_pool_map = torch.sum(torch.sum(l3_pool_map, 2),2)\n",
    "                \n",
    "                \n",
    "                # computing avg pool @ scale 1\n",
    "                # ---------------------------\n",
    "                avg_pl_l1 =  nn.Sequential(*[nn.AvgPool2d((l1_h,l1_w), stride=l1_s)])\n",
    "                avg_l1_pool_map = avg_pl_l1(fmap)\n",
    "                avg_l1_pool_map = torch.sum(torch.sum(avg_l1_pool_map, 2),2)\n",
    "                \n",
    "                # computing avg pool @ scale 2\n",
    "                # ---------------------------\n",
    "                avg_pl_l2 =  nn.Sequential(*[nn.AvgPool2d((l2_h,l2_w), stride=l2_s)])\n",
    "                avg_l2_pool_map = avg_pl_l2(fmap)\n",
    "                avg_l2_pool_map = torch.sum(torch.sum(avg_l2_pool_map, 2),2)\n",
    "                \n",
    "                # computing avg pool @ scale 3\n",
    "                # ---------------------------\n",
    "                avg_pl_l3 =  nn.Sequential(*[nn.AvgPool2d((l3_h,l3_w), stride=l3_s)])\n",
    "                avg_l3_pool_map = avg_pl_l3(fmap)\n",
    "                avg_l3_pool_map = torch.sum(torch.sum(avg_l3_pool_map, 2),2)\n",
    "                \n",
    "                \n",
    "                # NOT concatenating -- summing\n",
    "                # ----------------------------\n",
    "                combined_pool_map = l1_pool_map + l2_pool_map + l3_pool_map + avg_l1_pool_map + avg_l2_pool_map + avg_l3_pool_map\n",
    "\n",
    "            else:\n",
    "                \n",
    "                assert 1 == 2,'Error: invalid pool mode'\n",
    "                \n",
    "            \n",
    "            # 1.1.1.3 finally concatenating pool maps across all input feature maps\n",
    "            # ---------------------------------------------------------------------\n",
    "            try:\n",
    "                all_layers_pool_map = torch.cat((all_layers_pool_map,combined_pool_map), 1)\n",
    "            except:\n",
    "                all_layers_pool_map = combined_pool_map\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        # if the input dims are given\n",
    "        # ---------------------------\n",
    "        else:\n",
    "            \n",
    "            \n",
    "            # 1.1.2 dims is given as (f,s)\n",
    "            # ----------------------------\n",
    "            curr_f = kernel_dims[0]\n",
    "            curr_s = kernel_dims[1]\n",
    "            \n",
    "            # 1.1.2.1 setting up sequentials\n",
    "            # ------------------------------\n",
    "            mxpl =  nn.Sequential(*[nn.MaxPool2d((curr_f,curr_f), stride=curr_s)])\n",
    "            avgpl =  nn.Sequential(*[nn.AvgPool2d((curr_f,curr_f), stride=curr_s)])\n",
    "            \n",
    "            \n",
    "            # 1.1.2.2 forward pass to get pool maps\n",
    "            # --------------------------------------\n",
    "            if pool_mode == 'max':\n",
    "                \n",
    "                # computing max pool\n",
    "                # ------------------\n",
    "                pool_map = mxpl(fmap)\n",
    "\n",
    "            \n",
    "            elif pool_mode == 'avg':\n",
    "                \n",
    "                # computing avg pool\n",
    "                # ------------------\n",
    "                pool_map = avgpl(fmap)\n",
    "            \n",
    "            elif pool_mode == 'both':\n",
    "                \n",
    "                # computing both\n",
    "                # --------------\n",
    "                mx_pool_map = mxpl(fmap)\n",
    "                avg_pool_map = avgpl(fmap)\n",
    "                \n",
    "                # concatenating\n",
    "                # -------------\n",
    "                #pool_map = torch.cat((mx_pool_map,avg_pool_map), 1)\n",
    "                pool_map = mx_pool_map + avg_pool_map\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                assert 1 == 2,'Error: invalid pool mode'\n",
    "            \n",
    "            \n",
    "            \n",
    "            # 1.1.2.3 aggregating\n",
    "            # -------------------\n",
    "            if aggregate_pool_maps == True:\n",
    "                \n",
    "                # summing along h,w axises - pool_map will be of shape (m,no_channels)\n",
    "                # --------------------------------------------------------------------\n",
    "                pool_map = torch.sum(torch.sum(pool_map, 2),2)\n",
    "                \n",
    "            \n",
    "            # final resize before concat\n",
    "            # --------------------------\n",
    "            pool_map = pool_map.view(pool_map.size()[0],-1)\n",
    "            \n",
    "            \n",
    "            # final concat along channel axis\n",
    "            # --------------------------------\n",
    "            try:\n",
    "                all_layers_pool_map = torch.cat((all_layers_pool_map,pool_map), 1)\n",
    "            except:\n",
    "                all_layers_pool_map = pool_map\n",
    "                \n",
    "\n",
    "    # final return\n",
    "    # ------------\n",
    "    return all_layers_pool_map\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to simply return similar images based on whole latents\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "def similarity(input_latents_list,db_latents_list,xin,xdb,sim_weights,no_suggestions,similarity_check_mode,print_result):\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    1. input includes\n",
    "    \n",
    "    a. input images & db images latent lists\n",
    "    b. settings such as weights, mode etc\n",
    "    c. will return indices of db images sorted in descending order of ranking similarity\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 1. initialisations\n",
    "    # ------------------\n",
    "    interim_sim_array = {}\n",
    "    final_sim_array = {}\n",
    "    final_all_indices = []\n",
    "    similar_products = []\n",
    "    all_similarity_values = []\n",
    "    epsilon = 0.0001\n",
    "    \n",
    "    # 2. sanity assertions\n",
    "    # --------------------\n",
    "    assert len(input_latents_list) == len(db_latents_list), 'Error: length of input & db lists dont match.'\n",
    "    if sim_weights == 'equal':\n",
    "        \n",
    "        # setting sim weight values to 1\n",
    "        # ------------------------------\n",
    "        sim_weights = []\n",
    "        for _ in range(len(input_latents_list)):\n",
    "            sim_weights.append(1.0)\n",
    "    else:\n",
    "        assert len(sim_weights) == len(input_latents_list), 'Error: number of weights & length of latent lists dont match.'\n",
    "        #assert round(sum(sim_weights),2) == 1.0, 'Error: the weights dont sum to 1.0'\n",
    "    \n",
    "    # 3. looping through the INPUT latents list\n",
    "    # -----------------------------------------\n",
    "    print('1. looping through latent list..')\n",
    "    for l_counter in range(len(input_latents_list)):\n",
    "        \n",
    "        # 3.0 local initialisations\n",
    "        # -------------------------\n",
    "        curr_input_latent = input_latents_list[l_counter]\n",
    "        db_input_latent = db_latents_list[l_counter]\n",
    "        \n",
    "        \n",
    "        # 3.1 itering through every image in curr_input_latent\n",
    "        # ----------------------------------------------------\n",
    "        for i in range(curr_input_latent.shape[0]):\n",
    "            \n",
    "            # Finding similarity per example\n",
    "            # ------------------------------\n",
    "            curr_input_latent_example = curr_input_latent[i]\n",
    "            \n",
    "            \n",
    "            # checking using similarity\n",
    "            # -------------------------\n",
    "            if similarity_check_mode == 'l2':\n",
    "                \n",
    "                # using L2\n",
    "                # --------\n",
    "                similarity_array = np.sqrt(np.sum((curr_input_latent_example-db_input_latent)**2, axis = 1))\n",
    "                \n",
    "                \n",
    "            elif similarity_check_mode == 'cosine_ratio':\n",
    "                \n",
    "                # using cosine_ratio\n",
    "                # ------------------\n",
    "                curr_input_latent_example = curr_input_latent_example.reshape(1,curr_input_latent_example.shape[0])\n",
    "                similarity_array = cosine_similarity_multi(curr_input_latent_example,db_input_latent)\n",
    "                similarity_array = similarity_array.reshape(similarity_array.shape[0],)\n",
    "                similarity_array += np.average((np.minimum(curr_input_latent_example,db_input_latent)/(np.maximum(curr_input_latent_example,db_input_latent) + epsilon)), axis = 1)\n",
    "            \n",
    "            \n",
    "            elif similarity_check_mode == 'cosine':\n",
    "                \n",
    "                # using cosine_ratio\n",
    "                # ------------------\n",
    "                curr_input_latent_example = curr_input_latent_example.reshape(1,curr_input_latent_example.shape[0])\n",
    "                similarity_array = cosine_similarity_multi(curr_input_latent_example,db_input_latent)\n",
    "                similarity_array = similarity_array.reshape(similarity_array.shape[0],)\n",
    "                \n",
    "            \n",
    "            \n",
    "            elif similarity_check_mode == 'ratio':\n",
    "                \n",
    "                # ratio only\n",
    "                # ----------\n",
    "                similarity_array = np.average((np.minimum(curr_input_latent_example,db_input_latent)/(np.maximum(curr_input_latent_example,db_input_latent) + epsilon)), axis = 1)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # invalid sim check mode\n",
    "                # ----------------------\n",
    "                assert 1 == 2, 'Error: invalid similarity check mode. This can be either \"cosine_ratio\" or \"ratio\" only.'\n",
    "            \n",
    "            \n",
    "            # appending to interim sim array for final weights application\n",
    "            # ------------------------------------------------------------\n",
    "            \n",
    "            # this dict will store all similarity values per example\n",
    "            # that is - \n",
    "            # interim_sim_array[0] = [sim_array_based_on_latent_list_0, sim_array_based_on_latent_list_1,...]\n",
    "            \n",
    "            try:\n",
    "                interim_sim_array[i].append(similarity_array)\n",
    "            except:\n",
    "                interim_sim_array[i] = []\n",
    "                interim_sim_array[i].append(similarity_array)\n",
    "    \n",
    "    \n",
    "    # 4. itering thru dict & applying weights\n",
    "    # ---------------------------------------\n",
    "    print('2. applying weights & appending final results..')\n",
    "    for keys in interim_sim_array:\n",
    "        \n",
    "        # deleting old value\n",
    "        # ------------------\n",
    "        try:\n",
    "            del similarity_array_final\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # itering thru list\n",
    "        # -----------------\n",
    "        for each_sim_array_index in range(len(interim_sim_array[keys])):\n",
    "            curr_weighted_array = interim_sim_array[keys][each_sim_array_index] * sim_weights[each_sim_array_index]\n",
    "            try:\n",
    "                similarity_array_final += curr_weighted_array\n",
    "            except:\n",
    "                similarity_array_final = curr_weighted_array\n",
    "                \n",
    "                \n",
    "        # similarity_array_final must be of shape (m,)\n",
    "        ###\n",
    "        \n",
    "        # continuation - no change here on\n",
    "        # --------------------------------\n",
    "        if similarity_check_mode == 'l2':\n",
    "            \n",
    "            # since in l2 distances are calculated and smaller dis = higher sim\n",
    "            # ------------------------------------------------------------------\n",
    "            sorted_indices = list(np.argsort(similarity_array_final))\n",
    "            \n",
    "        else:\n",
    "            sorted_indices = list(np.argsort(-1*similarity_array_final))\n",
    "        final_indices = sorted_indices[0:no_suggestions]\n",
    "        final_indices = [int(fi) for fi in final_indices]\n",
    "        final_all_indices.append(final_indices)\n",
    "        all_similarity_values.append(similarity_array[final_indices])\n",
    "        similar_products.append(xdb[final_indices])\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    # 3. showing if required\n",
    "    # ----------------------\n",
    "    if print_result == True:\n",
    "        \n",
    "        # itering\n",
    "        # -------\n",
    "        for i in range(xin.shape[0]):\n",
    "            \n",
    "            print('>> At image ' + str(i) + ' of around ' + str(xin.shape[0]) + '..')\n",
    "            print('** Input Image - ')\n",
    "            plt.figure(figsize=(2,2))\n",
    "            plt.imshow(xin[i])\n",
    "            plt.show()\n",
    "           \n",
    "            print('** Showing result images..')\n",
    "            fig=plt.figure(figsize=(25, 25))\n",
    "            columns = 5\n",
    "            rows = 10\n",
    "            \n",
    "            \n",
    "            for i_1 in range(similar_products[i].shape[0]):\n",
    "                \n",
    "                #print('** At image ' + str(i) + ' showing option number ' + str(i_1) + '**')\n",
    "                #print('Image index number: ' + str(final_all_indices[i][i_1]))\n",
    "                #print('Similarity value: ' + str(all_similarity_values[i][i_1]))\n",
    "                \n",
    "                if xdb.shape[3] > 1:\n",
    "                    img = similar_products[i][i_1]\n",
    "                    fig.add_subplot(rows, columns, i_1+1)\n",
    "                    plt.imshow(img)\n",
    "                    #plt.show()\n",
    "                else:\n",
    "                    img = similar_products[i][i_1,:,:,0]\n",
    "                    fig.add_subplot(rows, columns, i_1+1)\n",
    "                    plt.imshow(img, cmap='gray')\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "            print('\\n#########################################\\n')\n",
    "                \n",
    "            \n",
    "    \n",
    "    # 3. final return\n",
    "    # ---------------\n",
    "    return similar_products,all_similarity_values,final_all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a wrapper around similarity function since each and every image has different number of filtered results\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def similarity_wrapper(input_latents_list,db_latents_list,xin,xdb,similarity_check_mode,concept_inds_list):\n",
    "    \n",
    "    \n",
    "    # 0. initialistions\n",
    "    # -----------------\n",
    "    _,h,w,c = xin.shape\n",
    "    \n",
    "    \n",
    "    # 1. itering through every single image\n",
    "    # -------------------------------------\n",
    "    for i in range(xin.shape[0]):\n",
    "        \n",
    "        # filter ops\n",
    "        # itering and appenidng filtered fmaps to curr list\n",
    "        # ----------\n",
    "        curr_db_latents = [each[concept_inds_list[i]] for each in db_latents_list]\n",
    "        \n",
    "        # input latents ops\n",
    "        # -----------------\n",
    "        curr_input_latents = [each[i].reshape(1,each.shape[1]) for each in input_latents_list]\n",
    "        \n",
    "    \n",
    "        # running sim functions\n",
    "        # ---------------------\n",
    "        _,_,_ = similarity(curr_input_latents,curr_db_latents,xin[i].reshape(1,h,w,c),xdb[concept_inds_list[i]],'equal',50,similarity_check_mode,True)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class fcn_mask_proposal_UNET(nn.Module):\n",
    "    def __init__(self, in_channels, latent_softmax):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This is WNET model\n",
    "        # ------------------\n",
    "        \n",
    "        # Showing conv up sizes - \n",
    "        # --------------------------\n",
    "        # (95,95) -- Insize\n",
    "        \n",
    "        # @conv1 - (47, 47)\n",
    "        # @conv2 - (23, 23)\n",
    "        # @conv3 - (11, 11)\n",
    "        # @conv4 - (5, 5)\n",
    "        # @conv5 - (2, 2)\n",
    "    \n",
    "        # Followed by a an avg pool (2,2) to make this 1,1\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        nw_activation_conv = nn.ReLU() #nn.LeakyReLU(0.2) # nn.Tanh() nn.Softmax2d()\n",
    "        f = 3\n",
    "        s = 2\n",
    "        dropout_prob = 0.2\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        \n",
    "        # CONV Down layers\n",
    "        # ----------------\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 16\n",
    "        ct1 = nn.Conv2d(in_channels,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1) # 47x47\n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 32\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2) # 23x23\n",
    "        \n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 64\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nw_activation_conv\n",
    "        cl3 = [ct3,cb3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3) # 11x11\n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        conv4 = 128\n",
    "        ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        cb4 = nn.BatchNorm2d(conv4)\n",
    "        ca4 = nw_activation_conv\n",
    "        cl4 = [ct4,cb4,ca4,dropout_node]\n",
    "        self.convl4 = nn.Sequential(*cl4) # 5x5\n",
    "        \n",
    "        # Conv 5\n",
    "        ###\n",
    "        conv5 = 256\n",
    "        ct5 = nn.Conv2d(conv4,conv5,f,stride = s)\n",
    "        cb5 = nn.BatchNorm2d(conv5)\n",
    "        ca5 = nw_activation_conv\n",
    "        cl5 = [ct5,cb5,ca5,dropout_node]\n",
    "        self.convl5 = nn.Sequential(*cl5) # 2x2\n",
    "        \n",
    "\n",
    "        # Pooling layer + softmax activation\n",
    "        # ----------------------------------\n",
    "        if latent_softmax == True:\n",
    "            avpl =  [nn.AvgPool2d((2,2), stride=1), nn.Softmax2d()]\n",
    "        else:\n",
    "            avpl =  [nn.AvgPool2d((2,2), stride=1)]\n",
    "        self.pool_net = nn.Sequential(*avpl)\n",
    "        \n",
    "      \n",
    "        # Transconv layers\n",
    "        # ----------------\n",
    "        # Showing conv up sizes - \n",
    "        # --------------------------\n",
    "        # Incoming input is 1 x 1 x C\n",
    "        # (5, 5)\n",
    "        # (11, 11)\n",
    "        # (23, 23)\n",
    "        # (47, 47)\n",
    "        # (95, 95)\n",
    "       \n",
    "        \n",
    "        # Upconv layer 0\n",
    "        ###\n",
    "        #up_conv0 = conv5\n",
    "        t0 = nn.ConvTranspose2d(conv5,conv5,2,stride = 1)\n",
    "        b0 = nn.BatchNorm2d(conv5)\n",
    "        a0 = nw_activation_conv\n",
    "        l0 = [t0,b0,a0,dropout_node]\n",
    "        self.upcl0 = nn.Sequential(*l0) # 2x2\n",
    "        \n",
    "        # Upconv layer 1\n",
    "        # concat layer\n",
    "        ###\n",
    "        #up_conv1 = conv4\n",
    "        t1 = nn.ConvTranspose2d(conv5,conv4,f,stride = s)\n",
    "        b1 = nn.BatchNorm2d(conv4)\n",
    "        a1 = nw_activation_conv\n",
    "        l1 = [t1,b1,a1,dropout_node]\n",
    "        self.upcl1 = nn.Sequential(*l1) # 5x5\n",
    "        \n",
    "        # Upconv layer 2\n",
    "        # concat layer\n",
    "        ###\n",
    "        #up_conv2 = conv3\n",
    "        t2 = nn.ConvTranspose2d(conv4,conv3,f,stride = s)\n",
    "        b2 = nn.BatchNorm2d(conv3)\n",
    "        a2 = nw_activation_conv\n",
    "        l2 = [t2,b2,a2,dropout_node]\n",
    "        self.upcl2 = nn.Sequential(*l2) # 11x11\n",
    "        \n",
    "        # Upconv layer 3\n",
    "        # concat layer\n",
    "        ###\n",
    "        #up_conv3 = conv2\n",
    "        t3 = nn.ConvTranspose2d(conv3,conv2,f,stride = s)\n",
    "        b3 = nn.BatchNorm2d(conv2)\n",
    "        a3 = nw_activation_conv\n",
    "        l3 = [t3,b3,a3,dropout_node]\n",
    "        self.upcl3 = nn.Sequential(*l3) # 23x23\n",
    "        \n",
    "        # Upconv layer 4\n",
    "        # concat layer\n",
    "        ###\n",
    "        #up_conv4 = conv1\n",
    "        t4 = nn.ConvTranspose2d(conv2,conv1,f,stride = s)\n",
    "        b4 = nn.BatchNorm2d(conv1)\n",
    "        a4 = nw_activation_conv\n",
    "        l4 = [t4,b4,a4,dropout_node]\n",
    "        self.upcl4 = nn.Sequential(*l4) # 47x47\n",
    "        \n",
    "        # Upconv layer 5 -- FINAL LAYER\n",
    "        # concat layer\n",
    "        ###\n",
    "        #up_conv5 = 1\n",
    "        t5 = nn.ConvTranspose2d(conv1,1,f,stride = s)\n",
    "        a5 = nn.Sigmoid()\n",
    "        l5 = [t5,a5]\n",
    "        self.upcl5 = nn.Sequential(*l5)\n",
    "    \n",
    "   \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Conv pass\n",
    "        # ---------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out)\n",
    "        c4_out = self.convl4(c3_out)\n",
    "        c5_out = self.convl5(c4_out)\n",
    "        \n",
    "        # pooling\n",
    "        # -------\n",
    "        latent_out = self.pool_net(c5_out)\n",
    "        \n",
    "        # Transconv pass\n",
    "        # --------------\n",
    "        f1_out = self.upcl0(latent_out)\n",
    "        f2_out = self.upcl1(f1_out + c5_out)\n",
    "        f3_out = self.upcl2(f2_out + c4_out)\n",
    "        f4_out = self.upcl3(f3_out + c3_out)\n",
    "        f5_out = self.upcl4(f4_out + c2_out)\n",
    "        f6_out = self.upcl5(f5_out + c1_out)\n",
    "        \n",
    "        return f6_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps to create a UNET AE to get embeddings\n",
    "# -------------------------------------------\n",
    "\n",
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class conv_classifier(nn.Module):\n",
    "    def __init__(self, in_channels, out_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        nw_activation_conv = nn.ReLU() #nn.LeakyReLU(0.2) # nn.Tanh() nn.Softmax2d()\n",
    "        f = 3\n",
    "        s = 2\n",
    "        dropout_prob = 0.10\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        \n",
    "        # CONV Down layers\n",
    "        # ----------------\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 16\n",
    "        ct1 = nn.Conv2d(in_channels,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1)\n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 32\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2)\n",
    "        \n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 64\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nw_activation_conv\n",
    "        cl3 = [ct3,cb3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3) \n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        conv4 = 128\n",
    "        ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        cb4 = nn.BatchNorm2d(conv4)\n",
    "        ca4 = nw_activation_conv\n",
    "        cl4 = [ct4,cb4,ca4,dropout_node]\n",
    "        self.convl4 = nn.Sequential(*cl4) # 5x5\n",
    "        \n",
    "        # Conv 5\n",
    "        ###\n",
    "        conv5 = 256\n",
    "        ct5 = nn.Conv2d(conv4,conv5,f,stride = s)\n",
    "        cb5 = nn.BatchNorm2d(conv5)\n",
    "        ca5 = nw_activation_conv\n",
    "        cl5 = [ct5,cb5,ca5,dropout_node]\n",
    "        self.convl5 = nn.Sequential(*cl5) # 2x2 here\n",
    "        \n",
    "        # pooling layer\n",
    "        # -------------\n",
    "        mx_avg_pl =  [nn.AvgPool2d((2,2), stride=1)]\n",
    "        self.pool_net = nn.Sequential(*mx_avg_pl) # 1x1 here\n",
    "        \n",
    "\n",
    "        # final out\n",
    "        # ---------\n",
    "        lnt1 = nn.Linear(conv5*1*1,out_size)\n",
    "        ln1 = [lnt1, nn.Sigmoid()]\n",
    "        self.linear1 = nn.Sequential(*ln1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 1. Conv pass down\n",
    "        # -----------------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out)\n",
    "        c4_out = self.convl4(c3_out)\n",
    "        c5_out = self.convl5(c4_out)\n",
    "        \n",
    "        # 2. GAP layer\n",
    "        # ------------\n",
    "        gap_pool_out = self.pool_net(c5_out)\n",
    "        \n",
    "        # 3. pred out\n",
    "        # -----------\n",
    "        pred_out = self.linear1(gap_pool_out.view(gap_pool_out.size()[0],-1))\n",
    "        \n",
    "        return pred_out\n",
    "    \n",
    "    \n",
    "    def score_cam_fmaps(self, x, layer):\n",
    "        \n",
    "        \n",
    "        # sanity\n",
    "        # ------\n",
    "        assert layer >=2 and layer <= 5,'Error: please choose layer between 2 and 5 only.'\n",
    "        \n",
    "        # 1. Conv pass down\n",
    "        # -----------------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out)\n",
    "        c4_out = self.convl4(c3_out)\n",
    "        c5_out = self.convl5(c4_out)\n",
    "        \n",
    "        # simple if else\n",
    "        # --------------\n",
    "        if layer == 2:\n",
    "            return c2_out\n",
    "        elif layer == 3:\n",
    "            return c3_out\n",
    "        elif layer == 4:\n",
    "            return c4_out\n",
    "        elif layer == 5:\n",
    "            return c5_out\n",
    "        else:\n",
    "            assert 1==2,'Error: something wrong. layer not in range 2-5.'\n",
    "        \n",
    "        \n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FCN class copied from image search notebook which worked\n",
    "# --------------------------------------------------------\n",
    "\n",
    "class fcn_ae_4_layer_std(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialising N/W here\n",
    "        # ---------------------\n",
    "        nw_activation_conv = nn.ReLU() #nn.LeakyReLU(0.2) # nn.Tanh() nn.Softmax2d()\n",
    "        f = 3\n",
    "        s = 2\n",
    "        dropout_prob = 0.1\n",
    "        dropout_node = nn.Dropout2d(p=dropout_prob)\n",
    "        \n",
    "        #  what worked - 3,64,128,256,128,64,3\n",
    "        \n",
    "        # Conv 1\n",
    "        ###\n",
    "        conv1 = 64\n",
    "        ct1 = nn.Conv2d(in_channels,conv1,f,stride = s)\n",
    "        cb1 = nn.BatchNorm2d(conv1)\n",
    "        ca1 = nw_activation_conv\n",
    "        cl1 = [ct1,cb1,ca1,dropout_node]\n",
    "        self.convl1 = nn.Sequential(*cl1)\n",
    "        \n",
    "        # Conv 2\n",
    "        ###\n",
    "        conv2 = 128\n",
    "        ct2 = nn.Conv2d(conv1,conv2,f,stride = s)\n",
    "        cb2 = nn.BatchNorm2d(conv2)\n",
    "        ca2 = nw_activation_conv\n",
    "        cl2 = [ct2,cb2,ca2,dropout_node]\n",
    "        self.convl2 = nn.Sequential(*cl2)\n",
    "        \n",
    "        # Conv 3\n",
    "        ###\n",
    "        conv3 = 256\n",
    "        ct3 = nn.Conv2d(conv2,conv3,f,stride = s)\n",
    "        cb3 = nn.BatchNorm2d(conv3)\n",
    "        ca3 = nw_activation_conv #nw_activation_conv\n",
    "        cl3 = [ct3,cb3,ca3,dropout_node]\n",
    "        self.convl3 = nn.Sequential(*cl3)\n",
    "        \n",
    "        # Conv 4\n",
    "        ###\n",
    "        conv4 = 512\n",
    "        ct4 = nn.Conv2d(conv3,conv4,f,stride = s)\n",
    "        cb4 = nn.BatchNorm2d(conv4)\n",
    "        ca4 = nn.Softmax2d()#nw_activation_conv #nn.Softmax2d() #nw_activation_conv\n",
    "        cl4 = [ct4,ca4,dropout_node]\n",
    "        self.convl4 = nn.Sequential(*cl4) # size 6 x 4\n",
    "        \n",
    "        \n",
    "        # Pooling layer\n",
    "        #mxpl =  [nn.MaxPool2d((2,2), stride=2)]\n",
    "        #avpl =  [nn.AvgPool2d((6,4), stride=1)]\n",
    "        #self.pool_net = nn.Sequential(*mxpl)\n",
    "        \n",
    "        # Adding a fully connected linear layer\n",
    "        #\n",
    "        \n",
    "        # Upconv layer 0\n",
    "        ###\n",
    "        #t0 = nn.ConvTranspose2d(conv4,conv4,2,stride = 2)\n",
    "        #b0 = nn.BatchNorm2d(conv4)\n",
    "        #a0 = nw_activation_conv\n",
    "        #l0 = [t0,b0,a0]\n",
    "        #self.upcl0 = nn.Sequential(*l0)\n",
    "        \n",
    "        # Upconv layer 1\n",
    "        ###\n",
    "        up_conv1 = 256\n",
    "        t1 = nn.ConvTranspose2d(conv4,up_conv1,f,stride = s)\n",
    "        b1 = nn.BatchNorm2d(up_conv1)\n",
    "        a1 = nw_activation_conv\n",
    "        l1 = [t1,b1,a1,dropout_node]\n",
    "        self.upcl1 = nn.Sequential(*l1)\n",
    "        \n",
    "        # Upconv layer 2\n",
    "        ###\n",
    "        up_conv2 = 128\n",
    "        t2 = nn.ConvTranspose2d(up_conv1,up_conv2,f,stride = s)\n",
    "        b2 = nn.BatchNorm2d(up_conv2)\n",
    "        a2 = nw_activation_conv\n",
    "        l2 = [t2,b2,a2,dropout_node]\n",
    "        self.upcl2 = nn.Sequential(*l2)\n",
    "        \n",
    "        # Upconv layer 3\n",
    "        ###\n",
    "        up_conv3 = 64\n",
    "        t3 = nn.ConvTranspose2d(up_conv2,up_conv3,f,stride = s)\n",
    "        b3 = nn.BatchNorm2d(up_conv3)\n",
    "        a3 = nw_activation_conv\n",
    "        l3 = [t3,b3,a3,dropout_node]\n",
    "        self.upcl3 = nn.Sequential(*l3)\n",
    "        \n",
    "        # Upconv layer 4\n",
    "        ###\n",
    "        t4 = nn.ConvTranspose2d(up_conv3,in_channels,f,stride = s)\n",
    "        a4 = nn.Sigmoid()\n",
    "        l4 = [t4,a4]\n",
    "        self.upcl4 = nn.Sequential(*l4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Generation\n",
    "        # ----------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out)\n",
    "        c4_out = self.convl4(c3_out)\n",
    "        #c5_out = self.pool_net(c3_out)\n",
    "        \n",
    "        #f1_out = self.upcl0(c5_out)\n",
    "        f2_out = self.upcl1(c4_out)\n",
    "        f3_out = self.upcl2(f2_out)\n",
    "        f4_out = self.upcl3(f3_out)\n",
    "        f5_out = self.upcl4(f4_out)\n",
    "        \n",
    "        return f5_out\n",
    "\n",
    "    \n",
    "    def latent(self, x):\n",
    "        \n",
    "        \n",
    "        # Generation\n",
    "        # ----------\n",
    "        c1_out = self.convl1(x)\n",
    "        c2_out = self.convl2(c1_out)\n",
    "        c3_out = self.convl3(c2_out)\n",
    "        c4_out = self.convl4(c3_out)\n",
    "            \n",
    "        \n",
    "        return c4_out\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END OF CODES\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL set ups for execution\n",
    "# -------------------------\n",
    "\n",
    "## SET UP CUDA OR NOT HERE + OTHER SET UPS\n",
    "##########################################\n",
    "\n",
    "dev_env = 'local' # 'gpu' or 'local'\n",
    "\n",
    "##########################################\n",
    "##########################################\n",
    "\n",
    "# Setting CUDA\n",
    "# ------------\n",
    "if dev_env == 'gpu':\n",
    "    use_cuda = True\n",
    "else:\n",
    "    use_cuda = False\n",
    "if use_cuda == True:\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "# SET FILE SPECIFIC NAMES HERE\n",
    "# ----------------------------\n",
    "if dev_env == 'gpu':\n",
    "    save_path = '/home/venkateshmadhava/codes/pmate2_localgpuenv/models/'\n",
    "    parent_url = '/home/venkateshmadhava/datasets/images'\n",
    "else:\n",
    "    save_path = '/Users/venkateshmadhava/Documents/pmate2/pmate2_env/models/'\n",
    "    parent_url = '/Users/venkateshmadhava/Documents/pmate2/local_working_files'\n",
    "\n",
    "\n",
    "# displaying save path\n",
    "# --------------------\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 1. mask proposal nw\n",
    "######################\n",
    "\n",
    "cn_file_name = 'fcn_unet_mpn_GRAY_nonsoftmax_256_5classpoc_bird_apple_fish_skull_star.tar'\n",
    "cn_save_path = save_path + cn_file_name\n",
    "model_mpn,_,epoch,loss,_ = load_saved_model_function(cn_save_path, use_cuda)\n",
    "print('epoch: ' + str(epoch))\n",
    "print('loss: ' + str(loss))\n",
    "model_mpn = model_mpn.eval()\n",
    "model_mpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 2. masked image classifier\n",
    "##############################\n",
    "\n",
    "cn_file_name = 'conv_masked_image_classifier_GRAY_5classpoc_bird_apple_fish_skull_star.tar'\n",
    "cn_save_path = save_path + cn_file_name\n",
    "model_mimgcls,_,epoch,loss,_ = load_saved_model_function(cn_save_path, use_cuda)\n",
    "print('epoch: ' + str(epoch))\n",
    "print('loss: ' + str(loss))\n",
    "model_mimgcls = model_mimgcls.eval()\n",
    "model_mimgcls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 3. AE\n",
    "########\n",
    "\n",
    "cn_file_name = 'ae_model_tboard_4_layer_512_softmax_RGB.tar'\n",
    "cn_save_path = save_path + cn_file_name\n",
    "model_ae_rgb,_,_,_,_ = load_saved_model_function(cn_save_path, use_cuda)\n",
    "model_ae_rgb = model_ae_rgb.eval()\n",
    "model_ae_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. loading base dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main set ups\n",
    "# ------------\n",
    "img_cls_h, img_cls_w = 95,95\n",
    "img_ae_h, img_ae_h = 175,175\n",
    "input_image_mode = 'gray'\n",
    "\n",
    "# setting up class labels first up\n",
    "# this needs to match with exact 'classTitle' in final_dict\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "global class_labels\n",
    "class_labels = ['Bird','Apples','Fish','Skulls','Star']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "# ------------\n",
    "base_folder = '/Users/venkateshmadhava/Desktop/db'\n",
    "\n",
    "\n",
    "# main op -- reading data for ae\n",
    "################################\n",
    "create_dataset_from_folder_all(base_folder,True,None,img_ae_h, img_ae_h)\n",
    "global x_images_dataset, x_images_dataset_gray, x_images_dataset_edge\n",
    "\n",
    "# setting up image\n",
    "# ---------------\n",
    "if input_image_mode == 'rgb':\n",
    "    x_db_ae = x_images_dataset\n",
    "elif input_image_mode == 'gray':\n",
    "    x_db_ae = x_images_dataset_gray\n",
    "elif input_image_mode == 'edge':\n",
    "    x_db_ae = x_images_dataset_edge\n",
    "else:\n",
    "    assert 1==2,'Error: invalid mode'\n",
    "    \n",
    "# sanity\n",
    "# ------\n",
    "x_db_rgb_ae = x_images_dataset\n",
    "print(x_db_ae.shape)\n",
    "\n",
    "\n",
    "# main op -- resizing data for classification\n",
    "############################################\n",
    "resize_pool(x_db_ae,img_cls_h,img_cls_w)\n",
    "global x_out\n",
    "x_db_cls = x_out\n",
    "print(x_db_cls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity viewing to ensure both datasets correspond well\n",
    "# ------------------------------------------------------\n",
    "randrange = random.sample(list(range(x_db_ae.shape[0])), 2)\n",
    "\n",
    "# showing\n",
    "# -------\n",
    "for i in randrange:\n",
    "    \n",
    "    print('95,95 image -- ')\n",
    "    plt.imshow(x_db_cls[i,:,:,0], cmap='gray')\n",
    "    plt.show()\n",
    "    print('175,175 image -- ')\n",
    "    plt.imshow(x_db_ae[i,:,:,0], cmap='gray')\n",
    "    plt.show()\n",
    "    print('********')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 db - extracting fmaps & predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up tensors\n",
    "# ------------------\n",
    "x_db_ae_trn = Variable(setup_image_tensor(x_db_ae)).float()\n",
    "x_db_cls_trn = Variable(setup_image_tensor(x_db_cls)).float()\n",
    "\n",
    "print(x_db_ae_trn.size())\n",
    "print(x_db_cls_trn.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting classification predictions\n",
    "# -------------------------------------\n",
    "\n",
    "# 1. getting mask\n",
    "# ---------------\n",
    "simple_forward_pass_pool(x_db_cls_trn,True,model_mpn)\n",
    "global y_out_global\n",
    "db_mask = y_out_global\n",
    "del y_out_global\n",
    "\n",
    "# 2. getting prediction\n",
    "# ---------------------\n",
    "simple_forward_pass_pool(x_db_cls_trn*db_mask.detach(),True,model_mimgcls)\n",
    "global y_out_global\n",
    "db_pred = y_out_global\n",
    "del y_out_global\n",
    "\n",
    "# sanity\n",
    "# ------\n",
    "print(db_pred.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extracting ae latents\n",
    "# ---------------------\n",
    "simple_forward_pass_pool(torch.cat((x_db_ae_trn,x_db_ae_trn,x_db_ae_trn), 1),True,model_ae_rgb.latent)\n",
    "global y_out_global\n",
    "db_ae_latent = y_out_global\n",
    "del y_out_global\n",
    "\n",
    "# sanity\n",
    "# ------\n",
    "print(db_ae_latent.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score cam viz on db set\n",
    "# --------------------------\n",
    "randrange = random.sample(list(range(x_db_cls_trn.size()[0])), 3)\n",
    "\n",
    "# setting up inputs\n",
    "# -----------------\n",
    "x_scam_in = x_db_cls_trn[randrange]\n",
    "x_scam_print = to_numpy_image(x_scam_in)\n",
    "\n",
    "# final score cam\n",
    "# ---------------\n",
    "_ = score_cam(model_mpn,model_mimgcls,x_scam_in,x_scam_print,3,'minmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. loading input folder dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "# ------------\n",
    "input_folder = '/Users/venkateshmadhava/Desktop/input'\n",
    "\n",
    "\n",
    "# main op -- reading data for ae\n",
    "################################\n",
    "create_dataset_from_folder_all(input_folder,True,None,img_ae_h, img_ae_h)\n",
    "global x_images_dataset, x_images_dataset_gray, x_images_dataset_edge\n",
    "\n",
    "# setting up image\n",
    "# ---------------\n",
    "if input_image_mode == 'rgb':\n",
    "    x_input_ae = x_images_dataset\n",
    "elif input_image_mode == 'gray':\n",
    "    x_input_ae = x_images_dataset_gray\n",
    "elif input_image_mode == 'edge':\n",
    "    x_input_ae = x_images_dataset_edge\n",
    "else:\n",
    "    assert 1==2,'Error: invalid mode'\n",
    "    \n",
    "# sanity\n",
    "# ------\n",
    "x_input_rgb_ae = x_images_dataset\n",
    "print(x_input_ae.shape)\n",
    "\n",
    "\n",
    "# main op -- resizing data for classification\n",
    "############################################\n",
    "resize_pool(x_input_ae,img_cls_h,img_cls_w)\n",
    "global x_out\n",
    "x_input_cls = x_out\n",
    "print(x_input_cls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity viewing to ensure both datasets correspond well\n",
    "# ------------------------------------------------------\n",
    "randrange = random.sample(list(range(x_input_cls.shape[0])), 2)\n",
    "\n",
    "# showing\n",
    "# -------\n",
    "for i in randrange:\n",
    "    \n",
    "    print('95,95 image -- ')\n",
    "    plt.imshow(x_input_cls[i,:,:,0], cmap='gray')\n",
    "    plt.show()\n",
    "    print('175,175 image -- ')\n",
    "    plt.imshow(x_input_ae[i,:,:,0], cmap='gray')\n",
    "    plt.show()\n",
    "    print('********')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 input - extracting fmaps & predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up tensors\n",
    "# ------------------\n",
    "x_input_ae_trn = Variable(setup_image_tensor(x_input_ae)).float()\n",
    "x_input_cls_trn = Variable(setup_image_tensor(x_input_cls)).float()\n",
    "\n",
    "print(x_input_ae_trn.size())\n",
    "print(x_input_cls_trn.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting classification predictions\n",
    "# -------------------------------------\n",
    "\n",
    "# 1. getting mask\n",
    "# ---------------\n",
    "simple_forward_pass_pool(x_input_cls_trn,True,model_mpn)\n",
    "global y_out_global\n",
    "input_mask = y_out_global\n",
    "del y_out_global\n",
    "\n",
    "# 2. getting prediction\n",
    "# ---------------------\n",
    "simple_forward_pass_pool(x_input_cls_trn*input_mask.detach(),True,model_mimgcls)\n",
    "global y_out_global\n",
    "input_pred = y_out_global\n",
    "del y_out_global\n",
    "\n",
    "# sanity\n",
    "# ------\n",
    "print(input_pred.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting ae latents\n",
    "# ---------------------\n",
    "simple_forward_pass_pool(torch.cat((x_input_ae_trn,x_input_ae_trn,x_input_ae_trn), 1),True,model_ae_rgb.latent)\n",
    "global y_out_global\n",
    "input_ae_latent = y_out_global\n",
    "del y_out_global\n",
    "\n",
    "# sanity\n",
    "# ------\n",
    "print(input_ae_latent.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score cam viz on input set\n",
    "# --------------------------\n",
    "randrange = random.sample(list(range(x_input_cls_trn.size()[0])), 3)\n",
    "\n",
    "# setting up inputs\n",
    "# -----------------\n",
    "x_scam_in = x_input_cls_trn[randrange]\n",
    "x_scam_print = to_numpy_image(x_scam_in)\n",
    "\n",
    "# final score cam\n",
    "# ---------------\n",
    "_ = score_cam(model_mpn,model_mimgcls,x_scam_in,x_scam_print,3,'minmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END OF LOADING AND EXTRACTING PREDICTIONS & FMAPS\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. search related codes begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 getting concepts filtered"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "notes\n",
    "-----\n",
    "\n",
    "1. filter db to include results that contain concepts in the input image\n",
    "2. not necessarily all concepts \n",
    "3. ie., if input has apples,star.. results must include images with single apple and single star as well but not star + tiger etc\n",
    "\n",
    "visual sim sort\n",
    "---------------\n",
    "1. based on filtered inds, just sort every result on the whole\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting inds of db set that have ONLY one or more concepts from input image\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "concept_inds_list =  return_filtered_concepts_inds(input_pred,db_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 building rmac latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# LATENT SETTINGS #################\n",
    "###################################################\n",
    "\n",
    "# what works RGB -- RGB local - (1/3*h,1/3*w) i.e if h,w of fmap is 10,10 then local pool dim is 3,3\n",
    "\n",
    "# latents settings\n",
    "# ----------------\n",
    "kernel_stride_dims = [(2,2),[5,2]]\n",
    "pool_mode = 'both'\n",
    "aggregate_pool_maps = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing db latents from fmaps\n",
    "# -------------------------------\n",
    "\n",
    "db_latents_list = final_latents([db_ae_latent],kernel_stride_dims,pool_mode,aggregate_pool_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing input latents from fmaps\n",
    "# ----------------------------------\n",
    "\n",
    "input_latents_list = final_latents([input_ae_latent],kernel_stride_dims,pool_mode,aggregate_pool_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# running simple wrapper\n",
    "# ----------------------\n",
    "\n",
    "similarity_wrapper(input_latents_list,db_latents_list,x_input_rgb_ae,x_db_rgb_ae,'ratio',concept_inds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
